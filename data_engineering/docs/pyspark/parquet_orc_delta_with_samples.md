
# ğŸ“¦ Why Use Parquet or ORC Instead of CSV/JSON?

## ğŸ§¾ Basic File Formats: CSV and JSON

### ğŸ“„ CSV (Row-Based, Text)
```
id,name,age
1,Alice,30
2,Bob,35
```

### ğŸ“„ JSON (Nested, Text)
```json
{"id": 1, "name": "Alice", "age": 30}
{"id": 2, "name": "Bob", "age": 35}
```

### âŒ Limitations
- Row-based â†’ Read everything even if you need one column
- No compression
- No predicate pushdown or column pruning

---

## ğŸ“Š Columnar Formats: Parquet and ORC

These formats store **columns together** for fast access.

---

## ğŸ§± Sample File Structure: Parquet

**Data is organized into:**
- Row groups (set of rows)
- Column chunks (columns stored together)
- Pages (encoded and compressed data)
- Metadata (min/max values, null counts)

```
[Parquet File]
â”œâ”€â”€ Row Group 1
â”‚   â”œâ”€â”€ Column: id
â”‚   â”œâ”€â”€ Column: name
â”‚   â””â”€â”€ Column: age
â”œâ”€â”€ Row Group 2
â”‚   â”œâ”€â”€ Column: id
â”‚   â”œâ”€â”€ Column: name
â”‚   â””â”€â”€ Column: age
â””â”€â”€ Footer Metadata
    â”œâ”€â”€ Schema
    â”œâ”€â”€ Compression info
    â””â”€â”€ Column statistics (min, max, nulls)
```

---

## ğŸ§± Sample File Structure: ORC

**Data is organized into:**
- Stripes (similar to Parquet's row groups)
- Index data (min/max values for each stripe)
- Data section (actual column data)
- Footer (metadata and statistics)

```
[ORC File]
â”œâ”€â”€ Stripe 1
â”‚   â”œâ”€â”€ Index Data
â”‚   â””â”€â”€ Column Data
â”œâ”€â”€ Stripe 2
â”‚   â”œâ”€â”€ Index Data
â”‚   â””â”€â”€ Column Data
â””â”€â”€ File Footer
    â”œâ”€â”€ Column Statistics
    â””â”€â”€ File Metadata
```

---

## Benefits of Parquet/ORC Over CSV/JSON with Explanations

| Feature             | Parquet / ORC âœ… | CSV / JSON âŒ | Why Is This Available in Parquet / ORC But Not in CSV / JSON? |
|---------------------|------------------|----------------|----------------------------------------------------------------|
| **Column Pruning**     | Yes              | No             | Parquet/ORC store data column-wise, allowing Spark to read only required columns. CSV/JSON are row-based, so the whole row must be read. |
| **Predicate Pushdown** | Yes              | No             | Parquet/ORC store column-level statistics (min/max/null counts), enabling Spark to skip irrelevant data. CSV/JSON donâ€™t store metadata. |
| **Compression**        | Built-in         | Limited        | Parquet/ORC support block-level compression and encoding (like RLE, Dictionary). CSV/JSON are just text and not optimized for compression. |
| **Parallel Reads**     | Yes              | Not optimal    | Columnar formats allow splits and multi-threaded reads. Row formats (CSV/JSON) have to be parsed line-by-line, slowing parallelism. |
| **[Schema Support](./schema_awareness_detailed.md)**     | Yes              | No             | Parquet/ORC embed full schemas and types. CSV has no types (everything is string); JSON is semi-structured but lacks strict typing. |

---

## ğŸ§  What is Predicate Pushdown?

Query:
```sql
SELECT name FROM people WHERE age > 30;
```

With **Parquet/ORC**:
- Skip reading rows where age â‰¤ 30 using metadata

With **CSV/JSON**:
- Load all rows, then apply the condition

---

## ğŸ“‰ What is Column Pruning?

- If you only need the `name` column:
  - Parquet/ORC â†’ Reads just the `name` column
  - CSV/JSON â†’ Reads the whole row

---

## âœ… Recommended Usage

| Use Case                   | Recommended Format |
|----------------------------|--------------------|
| Analytical queries         | Parquet or ORC     |
| Human-readable config/logs | JSON               |
| Small, simple exports      | CSV                |

---

## ğŸ Summary

- Parquet and ORC are **optimized for big data**
- Store data in **compressed**, **columnar**, and **schema-aware** format
- Enable **faster queries** via **predicate pushdown** and **column pruning**

---

## ğŸ§¬ What About Delta Lake?

**Delta Lake** is an open-source storage layer that brings **ACID transactions** and **schema enforcement** to big data lakes.

It works **on top of Parquet files**, providing additional features like:

- **ACID transactions**
- **Time travel** (query older versions of your data)
- **Schema evolution**
- **Efficient upserts and deletes**

---

## ğŸ§ª Using Delta Lake with PySpark and Blob Storage

### âœ… Recommended Setup

- **Storage**: Azure Blob Storage, AWS S3, GCS
- **Format**: Delta (internally uses Parquet)
- **API**: PySpark with Delta Lake

### ğŸ”§ Writing Delta Tables

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder     .appName("DeltaExample")     .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")     .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")     .getOrCreate()

df = spark.read.json("path/to/json/files")

# Write as Delta format
df.write.format("delta").mode("overwrite").save("wasbs://container@storageaccount.blob.core.windows.net/delta/people")
```

### ğŸ†• Delta Table Structure (on Blob Storage)

```
[delta/people]
â”œâ”€â”€ part-00000-*.snappy.parquet
â”œâ”€â”€ ...
â”œâ”€â”€ _delta_log/
â”‚   â”œâ”€â”€ 00000000000000000000.json  â† transaction log files
â”‚   â”œâ”€â”€ 00000000000000000001.json
â”‚   â””â”€â”€ ...
```

- Data stored as **Parquet**
- Metadata and transactions logged in `_delta_log`

---

## ğŸ§  Benefits of Delta Lake on Columnar Storage

| Feature             | Delta Lake          | Plain Parquet/ORC |
|---------------------|----------------------|-------------------|
| ACID transactions   | âœ… Yes               | âŒ No              |
| Time travel         | âœ… Yes               | âŒ No              |
| Schema enforcement  | âœ… Yes               | âŒ No              |
| Upserts & deletes   | âœ… Yes               | âŒ No (rewrite needed) |
| Blob compatibility  | âœ… Yes (Azure, S3)   | âœ… Yes             |

---

## ğŸ Summary: Why Delta + Parquet on Blob?

Delta Lake:
- Builds on **columnar speed** of Parquet
- Adds **robustness** for real-time and ETL pipelines
- Seamlessly integrates with **cloud blob storage**

Perfect for production-grade, scalable **data lakes**.

---

## ğŸ§± File Structure + Sample Data (Parquet vs ORC)

### ğŸ“¦ Parquet Template + Sample

**File Structure:**
```
[Parquet File]
â”œâ”€â”€ Row Group 1
â”‚   â”œâ”€â”€ Column: id
â”‚   â”œâ”€â”€ Column: name
â”‚   â””â”€â”€ Column: age
â””â”€â”€ Footer Metadata
    â”œâ”€â”€ Schema
    â”œâ”€â”€ Compression info
    â””â”€â”€ Column statistics (min, max, nulls)
```

**Sample Data Stored:**
```
Column: id   â†’ [1, 2, 3]
Column: name â†’ ["Alice", "Bob", "Carol"]
Column: age  â†’ [30, 35, 40]
```

- Columns are stored together â€” efficient for queries like `SELECT name`.

---

### ğŸ“¦ ORC Template + Sample

**File Structure:**
```
[ORC File]
â”œâ”€â”€ Stripe 1
â”‚   â”œâ”€â”€ Index Data
â”‚   â””â”€â”€ Column Data: id, name, age
â””â”€â”€ File Footer
    â”œâ”€â”€ Column Statistics
    â””â”€â”€ File Metadata
```

**Sample Data Stored:**
```
Column: id   â†’ [1, 2, 3]
Column: name â†’ ["Alice", "Bob", "Carol"]
Column: age  â†’ [30, 35, 40]

Index Data:
- age: min=30, max=40
- id: min=1, max=3
```

- ORC keeps index stats per stripe for predicate pushdown and fast filtering.

---

## âœ… Usage Summary

Both **Parquet** and **ORC**:
- Store columnar data like `[1, 2, 3]` per column
- Enable compression and fast filtering
- Best for analytical queries at scale

